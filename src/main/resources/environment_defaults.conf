env.name=ide
cluster.name=${component.name}"-"${env.name}
instance.name=node-0
instance.fqn=${cluster.name}"-"${instance.name}
log.path.current=/tmp/logs/${instance.fqn}
log.path.archive=/tmp/logs/${instance.fqn}/archive
log.level=INFO
sysout.detach=false
syserr.detach=false
daemon.pidfile=${log.path.current}/${instance.fqn}.pid
zkConString = "127.0.0.1:2181"
zkRootPath = "/config"
server.root =
{
type=com.askme.mandelbrot.server.RootServer
port=9999
host="127.0.0.1"
timeout=5
actorSystem {
  name=${cluster.name}"-akka"
  akka {
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    loglevel = INFO
  }
  spray {
    can.server {
      server-header = ${component.name}
      remote-address-header=on
      request-timeout=5 s
      idle-timeout=20 s
      pipelining-limit = 5
      stats-support = off
      raw-request-uri-header = on
      parsing {
        max-uri-length = 10k
        illegal-header-warnings = off
      }
    }

    routing {
      relaxed-header-parsing = on
    }
  }
}

kafka {
  producer {
    client.id=${instance.fqn}
    metadata.broker.list="172.30.30.203:9082,172.30.30.204:9082,172.30.30.165:9082"
    serializer.class=kafka.serializer.StringEncoder
  }
  consumer {
    conf = {
      consumer.id=${instance.fqn}
      group.id=${cluster.name}
      zookeeper.connect="172.30.30.203:2181,172.30.30.204:2181,172.30.30.165:2181"
      zookeeper.session.timeout.ms=1000
      auto.commit.interval.ms=1000
      deserializer.class=kafka.serializer.StringDecoder
    }
    topics = ["suggestion2"]
  }
  zookeeper {
    connect = "172.30.30.203:2181,172.30.30.204:2181,172.30.30.165:2181"
    session-timeout=1000000
    connect-timeout=1000
  }
  topics = [
    {
      name = "suggestion2"
      partitions = 5
      replication = 2
      conf = {
      }
    }
  ]
}

es {
  cluster.name = ${cluster.name}
  node {
    name = ${instance.name}
    data = true
    master = true
  }
  discovery.zen {
    ping.multicast.enabled = false
    ping.unicast.hosts = ${server.root.host}
    minimum_master_nodes = 1
  }
  http {
    compression = true
    max_content_length=1024mb
  }
  network.host = "127.0.0.1"
  path {
    data = "/tmp/data/es"
    logs = ${log.path.current}
    conf = "/tmp/esconfig"
    home = "/"
  }
  indices {
    queries.cache.size: "2048mb"
    fielddata.cache.size: "2048mb"
    memory.index_buffer_size: "2048mb"
    memory.max_index_buffer_size: "2048mb"
    memory.min_index_buffer_size: "2048mb"
    requests.cache.size: "1024mb"
  }
  gateway {
    recover_after_nodes: 1
    expected_nodes: 1
    recover_after_time: 2m
  }

  plugin {
    types: [
      "com.askme.mandelbrot.es.analysis.misc.RecombiningTokenFilterPlugin",
      "com.askme.mandelbrot.es.analysis.stem.StemmingTokenFilterPlugin",
      "com.askme.mandelbrot.scripts.ScriptRegistrationPlugin",
      "org.elasticsearch.index.reindex.ReindexPlugin"
    ]
  }
}

hazel {
  logging.type=slf4j
  multicast.enabled=false
  tcpip.enabled=true
  tcpip.members=[${server.root.host}]
  tcpip.required.member=${server.root.host}
  port.number=5701
  port.autoincrement=false
  interfaces=["192.168.*.*"]
  interface.enabled=true
}

spark {
  master="local[4]"
  app.name=${instance.fqn}
  executor.memory="1g"
  shuffle.spill="true"
  logConf="true"
  local.dir="/tmp"
  streaming.batch.duration=2
}

handler {
  name=${instance.fqn}"-http"
  timeoutms=5000
  max-docs-per-shard=500000
  indexing.enabled = false
  aggregate.enabled = true
  search {
    block {
      ip=["91.239.66.33", "157.55.39.55", "115.246.167.90", "1.39.62.69","151.80.31.140","1.39.35.14", "50.22.144.34",
        "172.30.2.34", "120.61.53.48", "43.247.158.4", "208.123.223.201", "49.15.132.164"]
      ip-prefix=["42.120."]
    }

    offset.min = 0
    offset.max = 500
    size.min = 0
    size.max = 500
    timeoutms.max = 10000
    kw-length.max = 200
    area-length.max = 100
  }
}

threads {
  batch=2
  user=8
}
}