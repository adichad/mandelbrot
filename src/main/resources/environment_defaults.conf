env.name=ide
cluster.name=${component.name}"-"${env.name}
instance.name=node-0
instance.fqn=${cluster.name}"-"${instance.name}
log.path.current=/tmp/logs/${instance.fqn}
log.path.archive=/tmp/logs/${instance.fqn}/archive
log.level=INFO
sysout.detach=false
syserr.detach=false
daemon.pidfile=${log.path.current}/${instance.fqn}.pid
server.root =
{
  type=com.askme.mandelbrot.server.RootServer
  port=9999
  host="127.0.0.1"
  timeout=5
  actorSystem {
    name=${cluster.name}"-akka"
    akka {
      loggers = ["akka.event.slf4j.Slf4jLogger"]
      loglevel = INFO
    }
    spray {
      can.server {
        server-header = ${component.name}
        remote-address-header=on
        request-timeout=5 s
        idle-timeout=20 s
        pipelining-limit = 5
        stats-support = off
        raw-request-uri-header = on
        parsing {
          max-uri-length = 10k
        }
      }

      routing {
        relaxed-header-parsing = on
      }
    }
  }

  kafka {
    producer {
      client.id=${instance.fqn}
      metadata.broker.list="172.30.30.203:9082,172.30.30.204:9082,172.30.30.165:9082"
      serializer.class=kafka.serializer.StringEncoder
    }
    consumer {
      conf = {
        consumer.id=${instance.fqn}
        group.id=${cluster.name}
        zookeeper.connect="172.30.30.203:2181,172.30.30.204:2181,172.30.30.165:2181"
        zookeeper.session.timeout.ms=1000
        auto.commit.interval.ms=1000
        deserializer.class=kafka.serializer.StringDecoder
      }
      topics = ["suggestion2"]
    }
    zookeeper {
      connect = "172.30.30.203:2181,172.30.30.204:2181,172.30.30.165:2181"
      session-timeout=1000000
      connect-timeout=1000
    }
    topics = [
      {
        name = "suggestion2"
        partitions = 5
        replication = 2
        conf = {
        }
      }
    ]
  }

  es {
    cluster.name=${cluster.name}
    node {
      name = ${instance.name}
      data = true
      master = true
    }
    discovery.zen {
      ping.multicast.enabled = false
      ping.unicast.hosts = ${server.root.host}
      minimum_master_nodes = 1
    }
    http {
      compression = true
    }
    network.host="127.0.0.1"
    path {
      data = "/tmp/data/es"
      logs = ${log.path.current}
      conf = "/tmp/esconfig"
      home = "/"
    }
    indices {
      queries.cache.size: "40%"
      cache.query.size: "20%"
      cache.filter.size: "40%"
      fielddata.cache.size: "2%"
      memory.index_buffer_size: "2048mb"
      memory.max_index_buffer_size: "2048mb"
      memory.min_index_buffer_size: "2048mb"
      store.throttle.max_bytes_per_sec: "20mb"
      store.throttle.type: "merge"
    }
    gateway {
      recover_after_nodes: 1
      expected_nodes: 1
      recover_after_time: 2m
    }
    script.native {
      geobucket.type = com.askme.mandelbrot.scripts.GeoBucket
      geobucketsuggest.type = com.askme.mandelbrot.scripts.GeoBucketSuggestions
      docscore.type = com.askme.mandelbrot.scripts.DocScore
      docscoreexponent.type = com.askme.mandelbrot.scripts.DocScoreExponent
      customertype.type = com.askme.mandelbrot.scripts.CustomerTypeBucket
      mediacount.type = com.askme.mandelbrot.scripts.MediaCount
      mediacountsort.type = com.askme.mandelbrot.scripts.MediaCountSort
      exactnamematch.type = com.askme.mandelbrot.scripts.ExactNameMatch
      suggestiontransform.type = com.askme.mandelbrot.scripts.SuggestionTransform
      curatedtag.type = com.askme.mandelbrot.scripts.CuratedTagComparator
      randomizer.type = com.askme.mandelbrot.scripts.RandomBucketComparator
    }

  }

  hazel {
    logging.type=slf4j
    multicast.enabled=false
    tcpip.enabled=true
    tcpip.members=[${server.root.host}]
    tcpip.required.member=${server.root.host}
    port.number=5701
    port.autoincrement=false
    interfaces=["192.168.*.*"]
    interface.enabled=true
  }

  spark {
    master="local[4]"
    app.name=${instance.fqn}
    executor.memory="1g"
    shuffle.spill="true"
    logConf="true"
    local.dir="/tmp"
    streaming.batch.duration=2
  }

  handler {
    name=${instance.fqn}"-http"
    timeoutms=5000
    max-docs-per-shard=500000
    indexing.enabled = false
    aggregate.enabled = true
    search {
      block {
        ip=["91.239.66.33", "157.55.39.55", "115.246.167.90", "1.39.62.69","151.80.31.140","1.39.35.14", "50.22.144.34",
          "172.30.2.34", "120.61.53.48", "43.247.158.4", "208.123.223.201", "49.15.132.164"]
        ip-prefix=["42.120."]
      }

      offset.min = 0
      offset.max = 600
      size.min = 0
      size.max = 500
      timeoutms.max = 10000
      kw-length.max = 200
      area-length.max = 100
    }
  }

  threads {
    batch=2
    user=8
  }
}
